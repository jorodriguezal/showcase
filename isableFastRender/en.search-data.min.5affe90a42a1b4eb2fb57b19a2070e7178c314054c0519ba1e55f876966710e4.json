[{"id":0,"href":"/showcase/docs/secciones/ilusiones-visuales/fen%C3%B3meno-de-masking/","title":"Fenómeno De Masking","section":"Ilusiones Visuales","content":"\rFenómeno de Masking (enmascaramiento)\r#\rEl masking visual es un fenómeno de percepción visual que se produce cuando la visibilidad de una imagen (objetivo) se ve reducida por la presencia de otra imagen (máscara). Implementación de kinegramas y patrones de Moiré como acercamiento al fenómeno visual de \u0026lsquo;masking\u0026rsquo;.\nKinegramas\r#\rUn kinegrama es un efecto de animación creado moviendo una superposición transparente a rayas a través de una imagen entrelazada. La palabra viene de \u0026ldquo;kine\u0026rdquo;, que significa \u0026ldquo;en movimiento\u0026rdquo;, y \u0026ldquo;-gram\u0026rdquo;, que significa \u0026ldquo;dibujo\u0026rdquo;.\nPatrones de Moiré\r#\rEs un patrón de interferencia que se forma cuando se superponen dos rejillas de líneas, ya sean rectas o curvas, con un cierto ángulo​ o cuando tales rejillas tienen tamaños ligeramente diferentes.\n"},{"id":1,"href":"/showcase/docs/secciones/ilusiones-visuales/introducci%C3%B3n/","title":"Introducción","section":"Ilusiones Visuales","content":"\rIntroducción\r#\rEn este primer informe académico se aborda el tema de ilusiones visuales, centrándose en cinco subtemas específicos: Coloración, Bandas de Mach, Enmascaramiento, Coherencia Espacial y Percepción de Profundidad. El objetivo principal de esta serie de ejercicios es explorar el proceso de percepción y representación de imágenes visuales en el cerebro humano y cómo ciertos patrones, colores y formas pueden engañar a nuestro sistema visual, generando ilusiones que no corresponden a la realidad. Este tema de investigación se seleccionó debido a la importancia de la comprensión del funcionamiento de la percepción visual en las aplicaciones informáticas, que pueden tener múltiples usos en diferentes áreas de la ingeniería. Además, las ilusiones visuales han sido objeto de estudio desde hace mucho tiempo, y su análisis puede proporcionar información valiosa acerca del funcionamiento de nuestro sistema perceptual. En este informe se presentará una descripción detallada de los subtemas desarrollados, explicando sus características y los distintos métodos para abordarlos y comprenderlos mejor. Así mismo, se presentarán ejemplos y experimentos para ilustrar cada uno de ellos, realizando distintos análisis con respecto a los resultados obtenidos.\rLa estructura del informe se divide en las secciones de introducción, métodos, resultados, discusión y conclusiones, donde cada sección se enfoca en un aspecto específico del estudio. En la sección de métodos se explicará la metodología utilizada para realizar los ejercicios. En la sección de resultados se presentarán los ejercicios desarrollados en funcionamiento y en la sección de discusión se analizarán e interpretarán los resultados, estableciendo posibles implicaciones y aplicaciones. "},{"id":2,"href":"/showcase/docs/secciones/integrantes/Johan-Steeb-Rodr%C3%ADguez-Alarc%C3%B3n/","title":"Johan Steeb Rodríguez Alarcón","section":"Integrantes","content":"\rJohan Steeb Rodríguez Alarcón\r#\rGitHub: jorodriguezal\n"},{"id":3,"href":"/showcase/docs/secciones/p5/iframe/","title":"Iframe","section":"P5","content":"p5 iframe shortcodes embed p5.js code within an iframe element. There are two p5 iframe shortcodes: p5-iframe and p5-global-iframe.\np5-iframe\r#\r{{\u0026lt; p5-iframe ver=\u0026#34;1.5.0\u0026#34; sketch=\u0026#34;/path/to/sketch.js\u0026#34; lib1=\u0026#34;https://cdntolib1/lib1.js\u0026#34; width=\u0026#34;800\u0026#34; height=\u0026#34;600\u0026#34; \u0026gt;}} All parameters are optional but sketch. Default values are shown in the above snippet but for libs*. Up to lib5 libs may be specified.\nColor relativity\r#\rLook at this brief explanation about what color relativity is.\np5-iframe markdown\r{{\u0026lt; p5-iframe sketch=\u0026#34;/showcase/sketches/monocular_cues/sketch.js\u0026#34; width=\u0026#34;725\u0026#34; height=\u0026#34;425 \u0026gt;}}\rThird party libraries\r#\rExample adapted from p5.EasyCam.\np5-iframe markdown\r{{\u0026lt; p5-iframe sketch=\u0026#34;/showcase/sketches/quick_easycam.js\u0026#34; lib1=\u0026#34;https://cdn.jsdelivr.net/gh/freshfork/p5.EasyCam@1.2.1/p5.easycam.min.js\u0026#34; width=\u0026#34;525\u0026#34; height=\u0026#34;525\u0026#34; \u0026gt;}}\rSound\r#\rExample took from the p5 examples.\np5-iframe markdown\r{{\u0026lt; p5-iframe sketch=\u0026#34;/showcase/sketches/sound.js\u0026#34; width=\u0026#34;225\u0026#34; height=\u0026#34;225\u0026#34; \u0026gt;}}\rp5-global-iframe\r#\r{{\u0026lt; p5-global-iframe id=\u0026#34;sketchid\u0026#34; ver=\u0026#34;1.5.0\u0026#34; lib1=\u0026#34;https://cdntolib1/lib1.js\u0026#34; width=\u0026#34;800\u0026#34; height=\u0026#34;600\u0026#34; \u0026gt;}} // inline sketch code {{\u0026lt; /p5-global-iframe \u0026gt;}} Note that the inline sketch should be coded in p5 global mode syntax.\rAll parameters are optional but id. Default values are shown in the above snippet but for libs*. Up to lib5 libs may be specified.\nBreathing square\r#\rLook at this reference for an explanation and further parameterization of the illusion.\np5-global-iframe markdown\r{{\u0026lt; p5-global-iframe id=\u0026#34;breath\u0026#34; width=\u0026#34;625\u0026#34; height=\u0026#34;625\u0026#34; \u0026gt;}} // Coded as `global mode` of [this](https://github.com/VisualComputing/Cognitive/blob/gh-pages/sketches/rotateSquare.js) let angle = 0; let speed = 0.06; function setup() { createCanvas(600, 600); } function draw() { background(255, 255, 255); rotateSquare(); if (!mouseIsPressed) { strokeWeight(0); stroke(0); fill(255, 140, 0); rect(0, 0, 281, 281); rect(318, 0, 281, 281); rect(0, 318, 281, 281); rect(318, 318, 281, 281); } } function rotateSquare() { push(); angle += speed; strokeWeight(0); stroke(0); fill(0, 0, 255); translate(width / 2, height / 2); rotate(angle); rect(-187.5, -187.5, 375, 375); pop(); } {{\u0026lt; /p5-global-iframe \u0026gt;}}\rp5-widget\r#\rThe p5-widget shortcode embed p5.js code within an p5-widget.\n{{\u0026lt; p5-widget autoplay=true height=\u0026#34;400\u0026#34; width=\u0026#34;400\u0026#34; ver=\u0026#34;1.5.0\u0026#34; \u0026gt;}} // inline sketch code {{\u0026lt; /p5-widget \u0026gt;}} All parameters are optional. Default ver is 1.5.0. For example:\nWidget example\r#\rp5-widget markdown\r{{\u0026lt; p5-widget autoplay=true height=\u0026#34;400\u0026#34; width=\u0026#34;400\u0026#34; ver=\u0026#34;1.5.0\u0026#34; \u0026gt;}} function setup() { createCanvas(300, 300); } function draw() { background(255, 0, 255); } {{\u0026lt; /p5-widget \u0026gt;}}\r"},{"id":4,"href":"/showcase/docs/secciones/ilusiones-visuales/Coherencia-espacial/","title":"Coherencia Espacial","section":"Ilusiones Visuales","content":"\rPixelamos un video\r#\r1.Antecedentes\r#\rLa técnica de pixelar imágenes ha sido utilizada desde hace décadas en diversos campos, como la fotografía, el cine, los videojuegos y el arte digital. Consiste en reducir la resolución de una imagen al agrupar los píxeles en bloques más grandes, dando como resultado una imagen con un aspecto más \"borroso\".\rEn los últimos años, ha surgido un enfoque denominado \"Spatial coherence\" (coherencia espacial), que se enfoca en preservar la información visual de la imagen original, mientras se reduce la resolución. En lugar de simplemente promediar los colores de cada bloque de píxeles, como se hace en la técnica de \"color averaging\", la técnica de \"spatial coherence\" utiliza un solo color arbitrario para cada bloque, con el objetivo de mantener la coherencia visual en toda la imagen.\rEsta técnica ha ganado popularidad en el ámbito del arte digital y la creación de gráficos por ordenador, ya que permite crear imágenes con un aspecto único y creativo, mientras se reduce la cantidad de información necesaria para representar la imagen. En esta página se tratará de comparar estas dos técnicas para una imagen o video\r2. Método\r#\rPara el desarrollo del ejercicio de hace uso de esta teoría ya descrita anteriormente de utiliza p5.js y se manejar imágenes y videos , para la técnica ColorAveraging se hace una media de los colores de cada bloque de píxeles determinado por un scrollbar y rápidamente se puede sacar el promedio de los nuevos bloques de pixeles para el siguiente fotograma del video. Para la técnica SpatialCoherence se utiliza un solo color arbitrario para cada bloque, con el objetivo de mantener la coherencia visual en toda la imagen.\rComo parte del ejercicio también se hizo uso de ChatGPT para ponerlo a prueba en qué tanto código podía generar, y se obtuvo que si bien no servían del todo los códigos generados si fue de ayuda para el desarrollo del resultado final del ejercicio. Se mostrarán fragmentos de código generador por la AI y cuáles de usaron y cuales no en la sección de fragmentos de código.\r3.Ejercicio\r#\rImplement a pixelator video application and perform a benchmark of the results (color avg vs spatial coherence). How would you assess the visual quality of the results?\r4. Resultados\r#\rImágenes\r#\rEn primer lugar se tiene el video que se puede observar en la siguiente imagen,en el cual se puede variar la cantidad de bloques de pixeles con un scrollbar, y se puede observar que al aumentar el tamaño de los bloques de pixeles(disminuir la cantidad de bloques), la imagen se vuelve más borrosa, y al disminuir el tamaño de los bloques de pixeles(aumentar la cantidad de bloques), la imagen se vuelve más nítida.\reste programa no consume tantos recursos por lo que corre bien con un video de alta calidad.\rcolor averaging\nspatial coherence\nVideos\r#\rcolor averaging\nspatial coherence\nSpatialCoherence\r#\rEn segundo lugar para esta parte del ejercicio no se ha logrado el programa con un video ya que es muy pesado y se queda sin recursos fácilmente,por lo que se ha hecho con una imagen, en la cual se puede observar que al aumentar el tamaño de los bloques de pixeles(disminuir la cantidad de bloques), la imagen se vuelve más borrosa, y al disminuir el tamaño de los bloques de pixeles(aumentar la cantidad de bloques), la imagen se vuelve más nítida.\rPara comparar los métodos se puede observar ambios programas con una imágen\r5. Fragmentos de código\r#\rAquí un ejemplo de fragmento de código generado por ChatGPT que no se utilizó en el ejercicio:\nFunción obtener color más frecuente\rComo se puede notar la función tiene una comploejidad muy alta y casi no se podía ejectar nisiquiera con una imágen, adicional a esto el ejercicio de spatial coherence juega es con un color aleatorio del cuadro que representa y no con el más común por lo que ya con los cambios requeridos implementados quedó así:\rFunción spathialCoherence\rfunction pixelateSpatialCoherence(originalImg) { // crear una nueva imagen pixelada let pixelatedImg = createImage(originalImg.width, originalImg.height); // cargar la imagen original en la nueva imagen pixelada pixelatedImg.copy(originalImg, 0, 0, originalImg.width, originalImg.height, 0, 0, originalImg.width, originalImg.height); // loop para recorrer cada bloque en la imagen pixelada for (let x = 0; x \u0026lt; pixelatedImg.width; x += blockSize) { for (let y = 0; y \u0026lt; pixelatedImg.height; y += blockSize) { // obtener un color aleatorio dentro del bloque let randX = floor(random(x, x + blockSize)); let randY = floor(random(y, y + blockSize)); let blockColor = originalImg.get(randX, randY); // loop para recorrer cada pixel en el bloque for (let i = x; i \u0026lt; x + blockSize; i++) { for (let j = y; j \u0026lt; y + blockSize; j++) { // asignar el color aleatorio al pixel en la imagen pixelada pixelatedImg.set(i, j, blockColor); } } } } Conclusiones\r#\r1. Para el desarrollo del video se requirió poner un video de baja calidad y pocos fotogramas ya que si se subía el programa no corría, especialmente con spathial coherence ya que el proceso de manera lineal cuesta mucho computacionalmente.\r2. Como se puede observar en los ejercicios el Pixerl averaging es más rápido que el spatial coherence, pero el spatial coherence es más preciso y se puede observar que la imagen se ve más nítida que la del pixel averaging, por lo que se puede concluir que el spatial coherence es mejor que el pixel averaging. 3. Para observar mejor la diferencia entre esos dos métodos se puede intentar palarelizar el proceso de spatial coherence, ya que el proceso es lineal es demasiado costoso computacionalmente, por lo que se puede intentar paralelizar el proceso para que sea más rápido y se pueda observar mejor la diferencia entre los dos métodos.\r4. ChatGPT es una herramienta muy útil para generar código, pero no es tan útil para generar código de manera automática, ya que el código generado no es muy útil y no se puede ejecutar, por lo que se debe hacer un proceso de limpieza y de adaptación del código generado para que sea útil y se pueda ejecutar.\r"},{"id":5,"href":"/showcase/docs/secciones/ilusiones-visuales/coloraci%C3%B3n/","title":"Coloración","section":"Ilusiones Visuales","content":"\rAntecedentes\r#\rLa percepción del color\r#\rLa percepción del color humana es una de las más complejas de las que se tiene conocimiento. Es un proceso que se lleva a cabo en el cerebro, y que se basa en la estimación de la longitud de onda de la luz que llega a los ojos. La longitud de onda de la luz visible oscila entre 380 y 750 nanómetros (nm). Esta se divide en tres bandas: roja, verde y azul. La luz que se encuentra en el rango de 380 a 450 nm se considera azul, la luz que se encuentra en el rango de 450 a 570 nm se considera verde, y la luz que se encuentra en el rango de 570 a 750 nm se considera roja. En el ojo humano, la luz que llega a la retina se divide en tres tipos de células: conos y bastones. Los conos son responsables de la percepción del color, mientras que los bastones son responsables de la percepción de la luminosidad. Los conos se dividen en tres tipos: conos rojos, verdes y azules. Cada uno de estos conos es sensible a una longitud de onda específica.\rLa información que llega a los conos se combina en el cerebro para generar la percepción del color. La información que llega a los conos rojos se combina con la información que llega a los conos verdes para generar la percepción del color amarillo. La información que llega a los conos verdes se combina con la información que llega a los conos azules para generar la percepción del color cian. Y la información que llega a los conos azules se combina con la información que llega a los conos rojos para generar la percepción del color magenta.\rDaltonismo El daltonismo es una condición que afecta a la percepción del color. Se caracteriza por la incapacidad de distinguir entre ciertos colores. En la mayoría de los casos, las personas con daltonismo no pueden distinguir entre el rojo y el verde. Se pueden distinguir los siguientes tipos de daltonismo:\rProtanopia : dificultad para percibir el color rojo. Deuteranopia : dificultad para percibir el color verde. Tritanopia : dificultad para percibir el color azul. Achromatopsia : incapacidad para percibir el color. Además se tienen las versiones más leves de estos tipos de daltonismo, que son:\nProtanomalia : dificultad leve para percibir el color rojo. Deuteranomalia : dificultad leve para percibir el color verde. Tritanomalia : dificultad leve para percibir el color azul. Achromatomalia : dificultad leve para percibir el color. Ejercicio realizado\r#\rMediante el mapeo de colores, se puede simular el daltonismo. Para ello, se analizan los colores que se encuentran en la imagen, y se reemplazan por los colores que se encuentran en la paleta de colores del daltonismo.\rMétodo utilizado\r#\rMapeo de colores para simular daltonismo En primer lugar, se sabe que, al igual que como se mencionó anteriormente, el color se puede dividir en tres bandas: roja, verde y azul. Este método es utilizado por los computadores para representar los colores, usando formatos como RGB, por lo cual, se facilita el análisis de los colores de la imagen.\rAdemás, se han planteado las matrices de transformación de colores, que permiten transformar los colores al multiplicar el vector de colores por estas. Para poder modificar los colores de la imagen y simular el daltonismo, se utilizan matrices de transformación que ya se han plantado para cada tipo de daltonismo. Estas se pueden encontrar en la siguiente página: https://gist.github.com/Lokno/df7c3bfdc9ad32558bb7 Finalmente, se recorre la imagen pixel por pixel, y se obtiene el color de cada uno de estos. Luego, se transforma el color a la paleta de colores del daltonismo ejegido, y se reemplaza el color original por el nuevo color.\rEvaluación de efectividad de filtros de color de Windows Microsoft Windows, en todas sus versiones, es uno de los sistemas operativos más utilizados en el mundo. En este, se pueden encontrar filtros de color que permiten que las personas con distintos tipos de daltonismo puedan ver mejor las diferencias entre los colores que les causan confusión. Para evaluar la efectividad de estos filtros, se utilizaron imágenes de referencia, a estas se les aplicó el filtro de color de Windows 11 y luego se les realizó la simulación de daltonismo con el método planteado anteriormente a todos los pares de imágenes para poder ver las diferencias entre la percepción de los colores sin el filtro y con el filtro.\rResultados\r#\rMapeo de colores para simular daltonismo\r#\rSe implementó el ejercicio en el lenguaje de programación JavaScript, utilizando la librería p5.js. Se puede ver el resultado a continuación:\nEn este, se tiene una imagen inicial de referencia con la que se pueden ver fácilmente las diferencias entre los tipos de daltonismo. También se cuenta con un desplegable que permite seleccionar el tipo de daltonismo que se desea simular y un botón que permite cargar una imagen distinta.\rLas matrices utilizadas se pueden observar en el siguiente código:\nMatrices de transformación de cada tipo de daltonismo\rvar colorMats = { \u0026#39;Normal\u0026#39;: [ [1,0,0], [0,1,0], [0,0,1] ], \u0026#39;Protanopia\u0026#39;: [ [0.567,0.433,0.000], [0.558,0.442,0.000], [0.000,0.242,0.758] ], \u0026#39;Protanomaly\u0026#39;: [ [0.817,0.183,0.000], [0.333,0.667,0.000], [0.000,0.125,0.875] ], \u0026#39;Deuteranopia\u0026#39;: [ [0.625,0.375,0.000], [0.700,0.300,0.000], [0.000,0.300,0.700] ], \u0026#39;Deuteranomaly\u0026#39;: [ [0.800,0.200,0.000], [0.258,0.742,0.000], [0.000,0.142,0.858] ], \u0026#39;Tritanopia\u0026#39;: [ [0.950,0.050,0.000], [0.000,0.433,0.567], [0.000,0.475,0.525] ], \u0026#39;Tritanomaly\u0026#39;: [ [0.967,0.033,0.000], [0.000,0.733,0.267], [0.000,0.183,0.817] ], \u0026#39;Achromatopsia\u0026#39;: [ [0.299,0.587,0.114], [0.299,0.587,0.114], [0.299,0.587,0.114] ], \u0026#39;Achromatomaly\u0026#39;: [ [0.618,0.320,0.062], [0.163,0.775,0.062], [0.163,0.320,0.516] ] } El código para la transformación de los colores se puede observar en el siguiente código:\nTransformación de los colores\rfunction draw() { ... matrix = colorMats[val]; newImg = createImage(img.width, img.height); newImg.loadPixels(); img.loadPixels(); for (let x = 0; x \u0026lt; img.width; x++) { for (let y = 0; y \u0026lt; img.height; y++) { let index = (x + y * img.width) * 4; let r = img.pixels[index]; let g = img.pixels[index + 1]; let b = img.pixels[index + 2]; let newR = matrix[0][0]*r + matrix[0][1]*g + matrix[0][2]*b; let newG = matrix[1][0]*r + matrix[1][1]*g + matrix[1][2]*b; let newB = matrix[2][0]*r + matrix[2][1]*g + matrix[2][2]*b; newImg.pixels[index] = newR; newImg.pixels[index + 1] = newG; newImg.pixels[index + 2] = newB; newImg.pixels[index + 3] = img.pixels[index + 3]; } }; } Con esto, se pudo simular el daltonismo en distintas imágenes, como se puede observar a continuación:\nComparativa de imágenes\rPaisaje original:\nPaisaje con daltonismo de tipo Protanopia:\nPaisaje con daltonismo de tipo Deuteranopia:\nPaisaje con daltonismo de tipo Tritanopia:\nPaisaje con daltonismo de tipo Achromatopsia:\nSi vemos un caso más extremo, podemos observar que esto puede generar dificultades para la identificación de distintos elementos en la imagen, como se puede observar en el siguiente ejemplo:\rimagen original:\nimagen con daltonismo de tipo Deuteranopia:\nFiltros de color de Windows aplicados a las imágenes\r#\rEstos fueron los resultados de pasar las imágenes confusas por los filtros de color de Windows y luego por el simulador de daltonismo:\nFiltros de color de Windows\rDeuteranopia:\nImagen original:\nImagen con daltonismo de tipo Deuteranopia:\nImagen con daltonismo de tipo Deuteranopia y filtro de color de Windows:\nTritanopia:\nImagen original:\nImagen con daltonismo de tipo Tritanopia:\nImagen con daltonismo de tipo Tritanopia y filtro de color de Windows:\nProtanopia:\nImagen original:\nImagen con daltonismo de tipo Protanopia:\nImagen con daltonismo de tipo Protanopia y filtro de color de Windows:\nComo se puede observar, los filtros de color de Windows en cierta medida ayudan a mejorar la visibilidad de las imágenes confusas, pero no son una solución definitiva para el problema, ya que no logran eliminar por completo la confusión en algunos casos.\rConclusiones y trabajo futuro\r#\rMediante este simple ejercicio de simulación se pudieron evidenciar las dificultades que pueden tener las personas con distintos tipos de daltonismo en su día a día. Esto se debe a que, al no poder distinguir ciertos colores, pueden tener dificultades para identificar objetos, personas, etc. Esto puede generar problemas en la vida cotidiana, como por ejemplo, en la conducción de vehículos, en la lectura de señales de tránsito, en la lectura de libros, etc. Por lo que este tipo de ejercicios nos ayudan a comprender mejor la problemática que enfrentan las personas con daltonismo y a generar soluciones que puedan ayudar a mejorar su calidad de vida.\rAdemás, aunque esta herramienta permite la identificación del problema, no es una solución definitiva, ya que, el siquiente paso sería generar una herramienta que permita la transformación de las imágenes de manera automática, para que las personas con daltonismo puedan visualizarlas sin dificultades.\rTambién se pudo evidenciar que, aunque los filtros de color de Windows ayudan a mejorar la visibilidad de las imágenes confusas, ampliando el contraste entre colores que pueden resultar conflictivos, no son una solución definitiva para el problema. Se requiere de herramientas más sofisticadas o más directas, como por ejemplo, las gafas de colores que se utilizan para la corrección de daltonismo, además de una mayor conciencia de los diseñadores y desarrolladores de interfaces de usuario para que tomen en cuenta a las personas con daltonismo a la hora de diseñar sus interfaces.\rReferencias\r#\rhttps://es.wikipedia.org/wiki/Daltonismo https://visualcomputing.github.io/docs/visual_illusions/coloring/ https://gist.github.com/Lokno/df7c3bfdc9ad32558bb7 https://www.es.colorlitelens.com/informacion-correccion-del-daltonismo.html#scientificbackground "},{"id":6,"href":"/showcase/docs/secciones/integrantes/Mar%C3%ADa-Alejandra-Jim%C3%A9nez-Herrera/","title":"María Alejandra Jiménez Herrera","section":"Integrantes","content":"\rMaría Alejandra Jiménez Herrera\r#\rGitHub: malejaj\n"},{"id":7,"href":"/showcase/docs/secciones/integrantes/Tania-Valentina-Castillo-Delgado/","title":"Tania Valentina Castillo Delgado","section":"Integrantes","content":"\rTania Valentina Castillo Delgado\r#\rGitHub: tvcastillod\n"},{"id":8,"href":"/showcase/docs/secciones/p5/div/","title":"Div","section":"P5","content":"p5 div shortcodes embed p5.js code within a div element. There are two p5 div shortcodes: p5-div and p5-instance-div.\np5-div\r#\r{{\u0026lt; p5-div ver=\u0026#34;1.5.0\u0026#34; sketch=\u0026#34;/path/to/sketch.js\u0026#34; lib1=\u0026#34;https://cdntolib1/lib1.js\u0026#34; \u0026gt;}} All parameters are optional but sketch. Default values are shown in the above snippet but for libs*. Up to lib5 libs may be specified.\nScintillating grid\r#\rLook at this and also this among many more references there are.\np5-div markdown\r{{\u0026lt; p5-div sketch=\u0026#34;/showcase/sketches/scintillating.js\u0026#34; \u0026gt;}}\rp5-instance-div\r#\r{{\u0026lt; p5-instance-div id=\u0026#34;sketchid\u0026#34; ver=\u0026#34;1.5.0\u0026#34; lib1=\u0026#34;https://cdntolib1/lib1.js\u0026#34; \u0026gt;}} // inline sketch code {{\u0026lt; /p5-instance-div \u0026gt;}} Note that the inline sketch should be coded in p5 instance mode syntax.\rAll parameters are optional but id. Default values are shown in the above snippet but for libs*. Up to lib5 libs may be specified.\nLilac chaser\r#\rLook at this introductory reference.\np5-instance-div markdown\r{{\u0026lt; p5-instance-div id=\u0026#34;lilac-chaser\u0026#34; \u0026gt;}} // Adapted from [this](https://github.com/VisualComputing/Cognitive/blob/gh-pages/sketches/lilacChaser.js) let jump = 0; let count = 0; p5.setup = function() { p5.createCanvas(400, 400); p5.frameRate(7); }; function drawBlurCircles(x, y, r) { p5.push(); p5.noStroke(); var opc = 0.4; var step = 3.0/r; for (var i = r; i \u0026gt; 0; i-=1.5) { if (opc \u0026lt; 5) { opc += step; p5.fill(255, 20, 180, opc); } p5.ellipse(x, y, i, i); } p5.pop(); }; p5.draw = function() { p5.background(200); var numCircles = 12; var stepAngle = 360.0/numCircles; p5.push(); p5.translate(p5.width/2.0, p5.height/2.0); for (var i = 0; i \u0026lt; 360; i = i + stepAngle) { if (i != jump) { p5.push(); p5.rotate(p5.radians(i)); drawBlurCircles(120, 0, 60); p5.pop(); } } if( !p5.mouseIsPressed ) { jump = (jump + stepAngle)%360; } p5.push(); p5.strokeWeight(1.5); p5.line(-7, 0, 7, 0); p5.line(0, -7, 0, 7); p5.pop(); p5.pop(); } {{\u0026lt; /p5-instance-div \u0026gt;}}\rNote that p5 should be the name to be used for the sketch object variable.\rVideo on canvas\r#\rAdapted from here. Don\u0026rsquo;t forget to checkout also the video on dom example.\np5-instance-div markdown\r{{\u0026lt; p5-instance-div id=\u0026#34;video\u0026#34; \u0026gt;}} let fingers; p5.setup = function() { p5.createCanvas(710, 400); // specify multiple formats for different browsers fingers = p5.createVideo([\u0026#39;/showcase/sketches/fingers.mov\u0026#39;, \u0026#39;/showcase/sketches/fingers.webm\u0026#39;]); fingers.hide(); // by default video shows up in separate dom // element. hide it and draw it to the canvas instead }; p5.draw = function() { p5.background(150); p5.image(fingers, 10, 10); // draw the video frame to canvas p5.filter(p5.GRAY); p5.image(fingers, 150, 150); // draw a second copy to canvas }; p5.mousePressed = function() { fingers.loop(); // set the video to loop and start playing } {{\u0026lt; /p5-instance-div \u0026gt;}}\rNote that p5 should be the name to be used for the sketch object variable.\r"},{"id":9,"href":"/showcase/docs/secciones/ilusiones-visuales/bandas-mach/","title":"Bandas Mach","section":"Ilusiones Visuales","content":"\rBandas Mach\r#\r1.Antecedentes\r#\rLa tpecnica de Mach Band es un eefecto óptico quese produce por la manera en que nuestro cerebro percibe diferentes tonalidades de color y luminosidad en unlos bordes de una imagen Fue descrita por primera vez por el fisiólogo Ernst Mach en 1865 y ha sido estudiada desde entonces por diversos investigadores en el campo de la percepción visual. El efecto consiste en la aparición de una banda oscura o clara a lo largo del borde entre dos áreas de diferentes niveles de luminosidad o color, lo que produce una apariencia de mayor contraste en la zona del borde.\rEsta técnica tiene implicaciones importantes en diversas áreas, como la psicología y la neurociencia, ya que permite estudiar cómo funciona la percepción visual en el cerebro humano y cómo ésta influye en nuestra percepción del mundo. Además, se ha utilizado en la industria gráfica y de la imagen para mejorar la calidad de las imágenes, reducir el efecto de posterización y mejorar la percepción del contraste.\rEn cuanto a la observación, podemos notar la presencia de este efecto en nuestra percepción visual cotidiana, en imágenes impresas, en televisores y en monitores de computadora, entre otros medios. La comprensión de esta técnica nos permite entender mejor cómo percibimos la realidad visualmente y cómo se pueden manipular las imágenes para mejorar su calidad y apariencia.\n2. Método\r#\rPara la realización de este ejercicio se utilizó el lenguaje de programación javascript , el cual permite la creación de gráficos y animaciones interactivas en 2D y 3D. Para la creación de la ilusión se utilizó la librería p5.js, con un decorador WEBGL que permite la creación de gráficos en 3D y se usó la técnica de Perlin noise para la generación de terrenos. El ruido Perlin es una funcón matemática que utiliza la interpolación entre varios grqdientes de vectores para generar una función de ruido. Esta función se utiliza para generar texturas, efectos de movimiento, terrenos, etc. ## 3.Ejercicio\rExercise Develop a terrain visualization application. Check out the 3D terrain generation with Perlin noise coding trainl tutorial .\r4. Resultados\r#\rEl programa crea una malla de puntos que se ajustan a los valores generados por la función de ruido Perlin. Además permite al usuario controlarla apariencia del terreno incluyendo el tamaño de los cuadrados de la malla, la velocidad de cambio del ruido Perlin y la capacidad de dibujar bandas de color para mejorar la percepción del relieve. También hay opciones para activar y desactivar el dibujo de líneas de contorno.\rEl cuadro \"Band\" permite mostrar o no las delimitaciones de los cuadros entonces si está seleccionado se dibujan los triángulos con diferentes colores en cada vértice y si no lo está se dibujan tiras con un solo tono de color para cada fila de puntos en el terreno.\rEl cuadro \"stroke\" permite mostrar o no las líneas de contorno del terreno.\rLos slider son el prmero para controlar la escala del terreno al cambiarlo se cambia el tamaño de los cuadros que conforman el terreno, a mayor tamaño menor será el detalle. Y el segundo slider controla la velocidad a la que el terreno cambia, la velocidad del ruiido Perlin se mueve en el eje Y.\r5. Fragmentos de código\r#\rSe hizo uso de dos funciones por separado para la habilitación o no de los checkbox del ejercicio, la primera función es la siguiente:\rFunción bands\rfunction verify_band(){ let s = scl.value(); if (band.checked()) { fill(200, 200, 200, 50); for (let y = 0; y \u0026lt; rows - 1; y++) { beginShape(TRIANGLES); for (let x = 0; x \u0026lt; cols - 2; x++) { fill((color[x][y] + color[x][y + 1] + color[x + 1][y]) / 3); vertex(x * s , y * s , terrain[x][y]); vertex(x * s , (y + 1) * s, terrain[x][y + 1]); vertex((x + 1) * s, y * s , terrain[x + 1][y]); fill((color[x + 1][y + 1] + color[x + 2][y] + color[x + 2][y + 1]) / 3); vertex((x + 1) * s, (y + 1) * s, terrain[x + 1][y + 1]); vertex((x + 2) * s, y * s , terrain[x + 2][y]); vertex((x + 2) * s, (y + 1) * s, terrain[x + 2][y + 1]); } endShape(); } } else { for (let y = 0; y \u0026lt; rows - 1; y++) { beginShape(TRIANGLE_STRIP); for (let x = 0; x \u0026lt; cols; x++) { fill(color[x][y]); vertex(x * s, y * s, terrain[x][y]); fill(color[x][y+1]); vertex(x * s, (y + 1) * s, terrain[x][y + 1]); } endShape(); } } } La segunda función es la siguiente:\rFunción stroke\rfunction verify_stroke(){ if (!strk.checked()) { noStroke(); } else { stroke(0, 255, 0); } } ``` "},{"id":10,"href":"/showcase/docs/secciones/ilusiones-visuales/percepci%C3%B3n-de-profundidad/","title":"Percepción De Profundidad","section":"Ilusiones Visuales","content":"\rAntecedentes\r#\rPercepción de la profundidad\r#\rLa percepción de profundidad es la habilidad de nuestro sistema visual para percibir la distancia relativa entre los objetos en un espacio tridimensional. Esta habilidad es importante para la percepción espacial y la navegación en el mundo real.\rPistas monoculares Las pistas monoculares son claves visuales que le permiten a nuestro sistema visual percibir la profundidad usando información de un solo ojo (a diferencia de las pistas binoculares, que facilitan la percepción de profundidad). Estas pistas incluyen el tamaño relativo de los objetos, la superposición de objetos, la textura, el gradiente de la perspectiva, entre otros. La combinación de estas pistas permite al sistema visual percibir la profundidad incluso cuando solo se tiene información de un solo ojo. En el contexto de la computación visual, las pistas monoculares se utilizan para crear ilusiones de profundidad en imágenes representadas en un espacio de dos dimensiones.\rEjercicio realizado\r#\rUtilizar la pistas monoculares para representar una escena tridimensional en un espacio de dos dimensiones.\rMétodo utilizado\r#\rPara la realización de este ejercicio se representó una escena simple, que consta de una carretera, una acera y una serie de árboles ubicados a distintas distancias, la cual se puede desplazar tanto horizontal como verticalmente. Además, se pusieron líneas de referencia para poder visualizar la profundidad de la escena.\rCon estos elementos se utilizaron distintas pistas monoculares, las cuales fueron:\nTamaño relativo de los objetos: Se estableció un tamaño relativo entre los objetos, de tal manera que el objeto más cercano se viera más grande y el más lejano más pequeño. Esto se logró con las líneas de referencia, las cuales se colocaron a diferentes distancias de la cámara, y con los árboles, los cuales se colocaron a diferentes distancias de la carretera.\rSuperposición de objetos: Se colocaron los árboles de tal manera que se superpongan entre sí, de tal manera que el árbol más cercano se vea más grande que el más lejano.\rGradiente de la perspectiva: Las líneas de la carretera, ya que son paralelas y se alejan de la cámara, tienden a converger en un punto en el horizonte.\rMovimiento Paralaje: Al moverse la cámara, los árboles se mueven a diferentes velocidades, de tal manera que los árboles más cercanos se mueven más rápido que los más lejanos.\rResultados\r#\rSe implementó el ejercicio en el lenguaje de programación JavaScript, utilizando la librería p5.js. El resultado se puede ver a continuación:\nPara destacar, en el código se plantearon dos variables que permiten modificar el punto de vista de la escena:\nlet finalLine; let XDeviation; Estas corresponden, respectivamente, al punto donde se dibuja el horizonte y la distancia horizontal entre la cámara y la carretera; y pueden ser modificadas en tiempo de ejecución por medio de sliders.\rAdemás, las distintas funciones que dibujan los objetos de la escena, como la carretera, la acera y los árboles, realizan sus cálculos de posición y tamaño en función de estas variables, de tal manera que se pueda modificar el punto de vista de la escena: Función que dibuja cada árbol\rfunction drawTree(img, posX, posY, sizeX, sizeY) { let newImg = img.get(); newImg.resize(sizeX, sizeY); image(newImg, posX, posY); } Función que dibuja las líneas de referencia\rfunction drawBackgroundLines(finalPos) { stroke(\u0026#34;#326133\u0026#34;); let posY = finalPos; let difference = height - finalPos; for (let i = 0; i \u0026lt;= 30; i++) { posY += ((difference / 30) * i) / 15; line(0, posY, width, posY); } return posY; } Función que colorea el cielo\rfunction drawSky(horizon) { noStroke(); fill(\u0026#34;#87ceeb\u0026#34;); beginShape(); vertex(0, 0); vertex(width, 0); vertex(width, horizon); vertex(0, horizon); endShape(); } Función que dibuja la carretera, la acera y los árboles\rfunction drawVanishingRoad(posY) { stroke(\u0026#34;black\u0026#34;); fill(\u0026#34;#5f5f5f\u0026#34;); //Vía beginShape(); vertex(width * (1 / 10) + XDeviation * 10, height); vertex(width / 2 - width / 150, posY); vertex(width / 2 + width / 150, posY); vertex(width * (9 / 10) + XDeviation * 10, height); endShape(); //Andénes fill(\u0026#34;#c4c4c4\u0026#34;); beginShape(); vertex(width * (1 / 10) + XDeviation * 10, height); vertex(width / 2 - width / 150, posY); vertex(0 + XDeviation * 10, height); endShape(); beginShape(); vertex(width * (9 / 10) + XDeviation * 10, height); vertex(width / 2 + width / 150, posY); vertex(width + XDeviation * 10, height); endShape(); fill(\u0026#34;white\u0026#34;); //líneas del centro beginShape(); vertex(width / 2 - 2 * (width * (1 / 40)) + XDeviation * 10, height); vertex(width / 2 - width / 1000, posY); vertex(width / 2 - width * (1 / 70) + XDeviation * 10, height); endShape(); beginShape(); vertex(width / 2 + 2 * (width * (1 / 40)) + XDeviation * 10, height); vertex(width / 2 + width / 1000, posY); vertex(width / 2 + width * (1 / 70) + XDeviation * 10, height); endShape(); //árboles if (treemode) { drawTree(img, width / 2.7 + XDeviation * 1.85, height - posY, 50, 50); drawTree( img, width / 2.7 + XDeviation * 1.85 + width * (4.15 / 24), height - posY, 50, 50 ); drawTree(img, width / 4 + XDeviation * 3.9, height - posY / 1, 100, 100); drawTree( img, width / 4 + XDeviation * 3.9 + width * (4.15 / 12), height - posY / 1, 100, 100 ); drawTree(img, XDeviation * 7.8, height - posY / 1, 200, 200); drawTree( img, XDeviation * 7.8 + width * (4.15 / 6), height - posY / 1, 200, 200 ); } } Conclusiones y trabajo futuro\r#\rEl ejercicio permitió comprender la forma en que el cerebro humano interpreta la información visual cuando no se tiene disponible todo el mecanismo de procesamiento binocular, y cómo esta información es procesada para generar una percepción de profundidad. Además, se pudo apreciar la importancia de las distintas pistas monoculares expuestas y la cercanía de la percepción visual con la realidad física.\rFinalmente, también se hace evidente las facilidades y ventajas de la computación visual para implementar y experimentar con diferentes escenarios de este tipo y así poder comprender mejor estos fenómenos y pensar en sus posibles aplicaciones en los distintos campos de la computación, como lo pueden ser la robótica, la inteligencia artificial o la realidad virtual.\rReferencias\r#\rhttps://visualcomputing.github.io/docs/visual_illusions/depth_perception/ https://es.wikipedia.org/wiki/Percepci%C3%B3n_de_profundidad "},{"id":11,"href":"/showcase/docs/secciones/ilusiones-visuales/","title":"Ilusiones Visuales","section":"Secciones","content":"\rPrimer reporte académico - Ilusiones visuales\r#\rFenómeno De Masking\rFenómeno de Masking (enmascaramiento)\r#\rEl masking visual es un fenómeno de percepción visual que se produce cuando la visibilidad de una imagen (objetivo) se ve reducida por la presencia de otra imagen (máscara). Implementación de kinegramas y patrones de Moiré como acercamiento al fenómeno visual de \u0026lsquo;masking\u0026rsquo;. Kinegramas\r#\rUn kinegrama es un efecto de animación creado moviendo una superposición transparente a rayas a través de una imagen entrelazada.\rIntroducción\rIntroducción\r#\rEn este primer informe académico se aborda el tema de ilusiones visuales, centrándose en cinco subtemas específicos: Coloración, Bandas de Mach, Enmascaramiento, Coherencia Espacial y Percepción de Profundidad. El objetivo principal de esta serie de ejercicios es explorar el proceso de percepción y representación de imágenes visuales en el cerebro humano y cómo ciertos patrones, colores y formas pueden engañar a nuestro sistema visual, generando ilusiones que no corresponden a la realidad.\rCoherencia Espacial\rPixelamos un video\r#\r1.Antecedentes\r#\rLa técnica de pixelar imágenes ha sido utilizada desde hace décadas en diversos campos, como la fotografía, el cine, los videojuegos y el arte digital. Consiste en reducir la resolución de una imagen al agrupar los píxeles en bloques más grandes, dando como resultado una imagen con un aspecto más \"borroso\".\rEn los últimos años, ha surgido un enfoque denominado \"Spatial coherence\" (coherencia espacial), que se enfoca en preservar la información visual de la imagen original, mientras se reduce la resolución.\rColoración\rAntecedentes\r#\rLa percepción del color\r#\rLa percepción del color humana es una de las más complejas de las que se tiene conocimiento. Es un proceso que se lleva a cabo en el cerebro, y que se basa en la estimación de la longitud de onda de la luz que llega a los ojos. La longitud de onda de la luz visible oscila entre 380 y 750 nanómetros (nm).\rBandas Mach\rBandas Mach\r#\r1.Antecedentes\r#\rLa tpecnica de Mach Band es un eefecto óptico quese produce por la manera en que nuestro cerebro percibe diferentes tonalidades de color y luminosidad en unlos bordes de una imagen Fue descrita por primera vez por el fisiólogo Ernst Mach en 1865 y ha sido estudiada desde entonces por diversos investigadores en el campo de la percepción visual. El efecto consiste en la aparición de una banda oscura o clara a lo largo del borde entre dos áreas de diferentes niveles de luminosidad o color, lo que produce una apariencia de mayor contraste en la zona del borde.\rPercepción De Profundidad\rAntecedentes\r#\rPercepción de la profundidad\r#\rLa percepción de profundidad es la habilidad de nuestro sistema visual para percibir la distancia relativa entre los objetos en un espacio tridimensional. Esta habilidad es importante para la percepción espacial y la navegación en el mundo real.\rPistas monoculares Las pistas monoculares son claves visuales que le permiten a nuestro sistema visual percibir la profundidad usando información de un solo ojo (a diferencia de las pistas binoculares, que facilitan la percepción de profundidad).\r"},{"id":12,"href":"/showcase/docs/secciones/integrantes/","title":"Integrantes","section":"Secciones","content":"\rIntegrantes del grupo\r#\rJohan Steeb Rodríguez Alarcón\rJohan Steeb Rodríguez Alarcón\r#\rGitHub: jorodriguezal\rMaría Alejandra Jiménez Herrera\rMaría Alejandra Jiménez Herrera\r#\rGitHub: malejaj\rTania Valentina Castillo Delgado\rTania Valentina Castillo Delgado\r#\rGitHub: tvcastillod\r"},{"id":13,"href":"/showcase/docs/secciones/p5/","title":"P5","section":"Secciones","content":"\rp5\r#\rp5 helps add test example p5 sketches into your book. There are two types of p5 shortcodes according to the html element used to embed them.\nTypes\r#\rIframe\rp5 iframe shortcodes embed p5.js code within an iframe element. There are two p5 iframe shortcodes: p5-iframe and p5-global-iframe. p5-iframe\r#\r{{\u0026lt; p5-iframe ver=\u0026#34;1.5.0\u0026#34; sketch=\u0026#34;/path/to/sketch.js\u0026#34; lib1=\u0026#34;https://cdntolib1/lib1.js\u0026#34; width=\u0026#34;800\u0026#34; height=\u0026#34;600\u0026#34; \u0026gt;}} All parameters are optional but sketch. Default values are shown in the above snippet but for libs*. Up to lib5 libs may be specified. Color relativity\r#\rLook at this brief explanation about what color relativity is. p5-iframe markdown\r{{\u0026lt; p5-iframe sketch=\u0026#34;/showcase/sketches/monocular_cues/sketch.\rDiv\rp5 div shortcodes embed p5.js code within a div element. There are two p5 div shortcodes: p5-div and p5-instance-div. p5-div\r#\r{{\u0026lt; p5-div ver=\u0026#34;1.5.0\u0026#34; sketch=\u0026#34;/path/to/sketch.js\u0026#34; lib1=\u0026#34;https://cdntolib1/lib1.js\u0026#34; \u0026gt;}} All parameters are optional but sketch. Default values are shown in the above snippet but for libs*. Up to lib5 libs may be specified. Scintillating grid\r#\rLook at this and also this among many more references there are. p5-div markdown\r{{\u0026lt; p5-div sketch=\u0026#34;/showcase/sketches/scintillating.\r"},{"id":14,"href":"/showcase/menu/","title":"Index","section":"Introduction","content":" Computación Visual Integrantes "}]